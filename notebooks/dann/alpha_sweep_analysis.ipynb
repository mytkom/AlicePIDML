{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the project directory\n",
    "%cd ../..\n",
    "# working directory should be ../pdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('src')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "SWEEP_METADATA_FILE = \"results/attention_dann_hyperparameter_tuning/sweep_118f437672375fa45c5e417106c304a1/sweep_metadata.json\"\n",
    "with open(SWEEP_METADATA_FILE, \"r\") as f:\n",
    "    sweep_metadata = json.load(f)\n",
    "\n",
    "alpha_parameters = defaultdict(list)\n",
    "\n",
    "for target_code, experiments in sweep_metadata.items():\n",
    "    for experiment in experiments:\n",
    "        base_dir = experiment[\"base_dir\"]\n",
    "        sweep_config = experiment[\"sweep_config\"]\n",
    "\n",
    "        alpha = sweep_config[\"model\"][\"attention_dann\"][\"alpha\"]\n",
    "\n",
    "        alpha_parameters[alpha].append(base_dir)\n",
    "\n",
    "print(\"Alpha Parameters:\")\n",
    "for alpha, dirs in alpha_parameters.items():\n",
    "    print(f\"  Alpha {alpha}: {dirs}\")\n",
    "\n",
    "save_dir = \"reports\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha_f1_scores = {}\n",
    "alpha_domain_accuracies = {}\n",
    "alpha_f1_stats = {}\n",
    "alpha_domain_stats = {}\n",
    "\n",
    "for alpha, base_dirs in alpha_parameters.items():\n",
    "    f1_scores = []\n",
    "    domain_accuracies = []\n",
    "\n",
    "    for base_dir in base_dirs:\n",
    "        metrics_path = os.path.join(base_dir, \"validation_metrics.csv\")\n",
    "        if not os.path.exists(metrics_path):\n",
    "            print(f\"Metrics file not found: {metrics_path}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(metrics_path)\n",
    "\n",
    "        if \"val/f1\" in df.columns and \"val/domain/accuracy\" in df.columns:\n",
    "            max_f1_row = df.loc[df[\"val/f1\"].idxmax()]\n",
    "            f1 = max_f1_row[\"val/f1\"]\n",
    "            domain_accuracy = max_f1_row[\"val/domain/accuracy\"]\n",
    "\n",
    "            f1_scores.append(f1)\n",
    "            domain_accuracies.append(domain_accuracy)\n",
    "\n",
    "    if f1_scores and domain_accuracies:\n",
    "        alpha_f1_stats[alpha] = (np.mean(f1_scores), np.std(f1_scores))\n",
    "        alpha_domain_stats[alpha] = (np.mean(domain_accuracies), np.std(domain_accuracies))\n",
    "\n",
    "sorted_alphas = sorted(alpha_f1_stats.keys())\n",
    "sorted_f1_means = [alpha_f1_stats[alpha][0] for alpha in sorted_alphas]\n",
    "sorted_f1_stds = [alpha_f1_stats[alpha][1] for alpha in sorted_alphas]\n",
    "sorted_domain_means = [alpha_domain_stats[alpha][0] for alpha in sorted_alphas]\n",
    "sorted_domain_stds = [alpha_domain_stats[alpha][1] for alpha in sorted_alphas]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(sorted_alphas, sorted_f1_means, yerr=sorted_f1_stds, fmt='-o', label=\"F1 Score\", capsize=5)\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score vs Alpha (with error bars)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(sorted_alphas, sorted_domain_means, yerr=sorted_domain_stds, fmt='-o', color='orange', label=\"Domain Accuracy\", capsize=5)\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Domain Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
