{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include source package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the project directory\n",
    "%cd ..\n",
    "# working directory should be ../FSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('src')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pdi.data.preparation import FeatureSetPreparation, MeanImputation, DeletePreparation, RegressionImputation, EnsemblePreparation\n",
    "from pdi.constants import PARTICLES_DICT, TARGET_CODES, NUM_WORKERS, P_RANGE, P_RESOLUTION\n",
    "from pdi.models import NeuralNet, NeuralNetEnsemble, AttentionModel, Traditional\n",
    "from pdi.data.types import Split, Additional\n",
    "from pdi.data.utils import DataPreparation\n",
    "from pdi.evaluate import calculate_precision_recall, get_predictions_data_and_loss, get_nsigma_predictions_data\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    #\"Mean\": {\n",
    "    #    \"model_class\": NeuralNet,\n",
    "    #    \"data\": {\n",
    "    #        \"all\": MeanImputation,\n",
    "    #        \"complete_only\": DeletePreparation,\n",
    "    #    }\n",
    "    #},\n",
    "    #\"Regression\": {\n",
    "     #   \"model_class\": NeuralNet,\n",
    "    #    \"data\": {\n",
    "     #      \"all\": RegressionImputation,\n",
    "    #        \"complete_only\": DeletePreparation,\n",
    "    #    }\n",
    "    #},\n",
    "    #\"Ensemble\": {\n",
    "    #    \"model_class\": NeuralNetEnsemble,\n",
    "    #    \"data\": {\n",
    "    #        \"all\": EnsemblePreparation,\n",
    "    #        \"complete_only\": lambda: EnsemblePreparation(complete_only=True),\n",
    "    #    }\n",
    "    #},\n",
    "    #\"Proposed\": {\n",
    "    #    \"model_class\": AttentionModel,\n",
    "    #    \"data\": {\n",
    "    #        \"all\": FeatureSetPreparation,\n",
    "    #        \"complete_only\": lambda: FeatureSetPreparation(complete_only=True),\n",
    "    #    }\n",
    "    #},\n",
    "    #\"Delete\": {\n",
    "    #    \"model_class\": NeuralNet,\n",
    "    #    \"data\": {\n",
    "    #        \"complete_only\": DeletePreparation,\n",
    "    #    }\n",
    "    #},\n",
    "    \"NSigma\": {\n",
    "        \"model_class\": Traditional,\n",
    "        \"data\": {\n",
    "            \"all\": DataPreparation,\n",
    "            \"complete_only\": DeletePreparation,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "particle_names = [PARTICLES_DICT[i] for i in TARGET_CODES]\n",
    "model_names = EXPERIMENTS.keys()\n",
    "metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "data_types = [\"all\", \"complete_only\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate pretrained models on test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "metric_results = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product(\n",
    "        [particle_names, model_names], names=[\"particle\", \"model\"]\n",
    "        ),\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [data_types, metrics], names=[\"data\", \"metric\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "prediction_data = {}\n",
    "\n",
    "for target_code in TARGET_CODES:\n",
    "    particle_name = PARTICLES_DICT[target_code]\n",
    "    for experiment_name, exp_dict in EXPERIMENTS.items():\n",
    "        if exp_dict[\"model_class\"] != Traditional:\n",
    "            load_path = f\"models/{experiment_name}/{particle_name}.pt\"\n",
    "            saved_model = torch.load(load_path)\n",
    "            model = exp_dict[\"model_class\"](*saved_model[\"model_args\"]).to(device)\n",
    "            model.thres = saved_model[\"model_thres\"]\n",
    "            model.load_state_dict(saved_model[\"state_dict\"])\n",
    "\n",
    "        batch_size = 512\n",
    "\n",
    "        prediction_data[experiment_name] = {}\n",
    "        for data_type, data_prep in exp_dict[\"data\"].items():\n",
    "            test_loader, = data_prep().prepare_dataloaders(batch_size, NUM_WORKERS, [Split.TEST])\n",
    "            \n",
    "            if exp_dict[\"model_class\"] != Traditional:\n",
    "                predictions, targets, add_data, _ = get_predictions_data_and_loss(model, test_loader, device)\n",
    "                selected = predictions > model.thres\n",
    "            else:\n",
    "                predictions, targets, add_data = get_nsigma_predictions_data(test_loader, target_code)\n",
    "                selected = predictions < 3.0\n",
    "\n",
    "            binary_targets = targets == target_code\n",
    "\n",
    "            true_positives = int(np.sum(selected & binary_targets))\n",
    "            selected_positives = int(np.sum(selected))\n",
    "            positives = int(np.sum(binary_targets))\n",
    "\n",
    "            precision, recall, _, _ = calculate_precision_recall(true_positives, selected_positives, positives)\n",
    "            f1 = 2 * precision * recall / (precision + recall + np.finfo(float).eps)\n",
    "\n",
    "            metric_results.loc[(particle_name, experiment_name), data_type] = precision, recall, f1\n",
    "            \n",
    "            prediction_data[experiment_name][data_type] = {\n",
    "                \"targets\": binary_targets,\n",
    "                \"predictions\": predictions,\n",
    "                \"momentum\": add_data[Additional.fP.name],\n",
    "                \"threshold\": model.thres\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split.TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"reports/tables\", exist_ok=True)\n",
    "metric_results.to_csv(f\"reports/tables/comparison_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot precision (purity) and recall (efficiency) comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pdi.visualise import (\n",
    "    plot_purity_comparison,\n",
    "    plot_efficiency_comparison,\n",
    "    plot_precision_recall_comparison,\n",
    ")\n",
    "\n",
    "p_min, p_max = P_RANGE\n",
    "p_range = np.linspace(p_min, p_max, P_RESOLUTION)\n",
    "intervals = list(zip(p_range[:-1], p_range[1:]))\n",
    "\n",
    "for target_code in TARGET_CODES:\n",
    "    particle_name = PARTICLES_DICT[target_code]\n",
    "    for data_type in data_types:\n",
    "        data = {}\n",
    "        for exp_name, exp_dict in prediction_data.items():\n",
    "            if data_type in exp_dict:\n",
    "                data[exp_name] = exp_dict[data_type]\n",
    "\n",
    "        save_dir = f\"reports/figures/comparison_{data_type}/{particle_name}\"   \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plot_purity_comparison(particle_name, data, intervals, save_dir)\n",
    "        plot_efficiency_comparison(particle_name, data, intervals, save_dir)\n",
    "        plot_precision_recall_comparison(particle_name, data, save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LaTeX table from metrics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"reports/tables/comparison_metrics.csv\",\n",
    "                 index_col=[0, 1],\n",
    "                 header=[0, 1])\n",
    "subsets = [((part, slice(None)), column) for column in df.columns\n",
    "           for part in particle_names]\n",
    "\n",
    "polish_model_names = {\n",
    "    \"Mean\": \"Średnia\",\n",
    "    \"Regression\": \"Regresja\",\n",
    "    \"Ensemble\": \"Zestaw\",\n",
    "    \"Proposed\": \"Prop. rozw.\",\n",
    "    \"Delete\": \"Usunięcie\",\n",
    "}\n",
    "\n",
    "polish_metrics = {\n",
    "    \"precision\": \"precyzja\",\n",
    "    \"recall\": \"czułość\",\n",
    "    \"f1\": \"f1\",\n",
    "}\n",
    "\n",
    "\n",
    "for dt in data_types:\n",
    "    save_dir = f\"reports/tables/comparison_{dt}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for particle in particle_names:\n",
    "        results = df.xs(particle).xs(dt, axis='columns')\n",
    "        results = results.rename(index=polish_model_names, columns=polish_metrics)\n",
    "        results.columns.name = \"metryka\"\n",
    "        results.index.name = \"model\"\n",
    "        # results.columns.rename(polish_metrics)\n",
    "        columns = [(slice(None), column) for column in polish_metrics.values()]\n",
    "        style = results.style\n",
    "        style.format(precision=4)\n",
    "        for column in columns:\n",
    "            style = style.highlight_max(column, props='textbf:--rwrap')\n",
    "        style.to_latex(f\"{save_dir}/{particle}_results.tex\",\n",
    "                       hrules=True,\n",
    "                       clines=\"all;data\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ddece17a642146cc49b2b032ef0865aafdc2c2bbdb5ddaf5cd80c99ab7aea91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
