{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the project directory\n",
    "%cd ..\n",
    "# working directory should be ../pdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('src')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use this notebook?\n",
    "1. Train models with desired configs and use `scripts` subdirectory scripts to achieve that.\n",
    "2. Fill `MODELS` dictionary with paths to the results dir of the run and name it appropriately as in dictionary element key.\n",
    "3. Run desired plot/table generation cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdi.constants import PART_NAME_TO_TARGET_CODE\n",
    "\n",
    "MODELS = {\n",
    "    \"Attention\": \"results/attention_hyperparameter_tuning_outlier_filtering/proton/isolation_forest_f1_0_86\",\n",
    "}\n",
    "target_code = PART_NAME_TO_TARGET_CODE[\"proton\"]\n",
    "\n",
    "save_dir = \"reports\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pdi.config import Config\n",
    "from pdi.engines import build_engine\n",
    "from pdi.results_and_metrics import TestResults\n",
    "from pdi.data.data_preparation import DataPreparation\n",
    "from pdi.data.types import Split\n",
    "\n",
    "data_prep: DataPreparation = None\n",
    "checksums = set()\n",
    "test_results: dict[str, TestResults] = {}\n",
    "for k, v in MODELS.items():\n",
    "    with open(f\"{v}/config.json\", 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "    config = Config.from_dict(config_data)\n",
    "    config.training.device = \"cpu\"\n",
    "    engine = build_engine(config, target_code, base_dir=v)\n",
    "    current_data_prep = engine.get_data_prep()\n",
    "    if data_prep is None:\n",
    "        data_prep = current_data_prep\n",
    "    checksums.add(current_data_prep._inputs_checksum)\n",
    "    test_results[k] = engine.test(model_dirpath=v)\n",
    "\n",
    "if len(checksums) > 1:\n",
    "    raise RuntimeError(\"You shouldn't compare models trained on different datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"nSigma\"] = data_prep.get_nsigma_test_results(target_code, threshold_unscaled=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Unwrapped and Unstandardized Test Split Data as a DataFrame\n",
    "The test split data is explicitly obtained from the `CombinedDataLoader` to ensure consistency. Only the `CombinedDataLoader` has the knowledge of how to unwrap itself, and it will raise errors if the operation cannot be performed. While this could also be achieved by adding an additional method in the `DataPreparation` class, doing so would require `DataPreparation` to understand the internal structure of the `CombinedDataLoader`. This approach would also necessitate updates to `DataPreparation` whenever changes are made to the `CombinedDataLoader`. Therefore, the current approach is preferred for maintaining separation of concerns and avoiding unnecessary dependencies. It is also thousands times faster than iterating over and over dataloader and concatenating batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = data_prep.create_dataloaders(\n",
    "    {\n",
    "        Split.TEST: 1 # not used\n",
    "    },\n",
    "    {\n",
    "        Split.TEST: 1 # not used\n",
    "    },\n",
    "    False, False)[Split.TEST]\n",
    "\n",
    "test_data_unwrapped = test_dl.unwrap()\n",
    "print(test_data_unwrapped.shape)\n",
    "test_data_unwrapped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate comparison table on optimal posterior probability threshold for f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics_data = []\n",
    "for model_name, test_result in test_results.items():\n",
    "    metrics = test_result.test_metrics.to_dict()\n",
    "    metrics['Model'] = model_name\n",
    "    metrics_data.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df)\n",
    "\n",
    "# Save as LaTeX table\n",
    "latex_table = metrics_df.to_latex(index=False)\n",
    "with open(f\"{save_dir}/test_metrics_comparison_{target_code}.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "# Save as CSV file\n",
    "metrics_df.to_csv(f\"{save_dir}/test_metrics_comparison_{target_code}.csv\", index=False)\n",
    "\n",
    "print(\"LaTeX table and CSV file saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision-recall curve for changing posterior probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdi.visualise import plot_precision_recall_comparison\n",
    "from pdi.data.data_exploration import generate_figure_thumbnails_from_iterator\n",
    "\n",
    "pt_ranges = [\n",
    "    (0., float(\"inf\")),\n",
    "    (0.,.5),\n",
    "    (.5,1.),\n",
    "    (1., 2.),\n",
    "    (2., float(\"inf\")),\n",
    "]\n",
    "\n",
    "figures = []\n",
    "\n",
    "for pt_range in pt_ranges:\n",
    "    mask = (test_data_unwrapped[\"fPt\"] >= pt_range[0]) & (test_data_unwrapped[\"fPt\"] < pt_range[1])\n",
    "    figures.append((plot_precision_recall_comparison(test_results, title_suffix=f\"p_t range [{pt_range[0]},{pt_range[1]})\", mask=mask.to_numpy()), f\"pt_{pt_range[0]}_{pt_range[1]}.png\"))\n",
    "\n",
    "generate_figure_thumbnails_from_iterator(figures, save_path=save_dir, thumbnail_width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics vs transverse momentum for optimal posterior probability threshold for f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdi.visualise import plot_metrics_vs_pt_comparison\n",
    "\n",
    "generate_figure_thumbnails_from_iterator(plot_metrics_vs_pt_comparison(test_results, pt=test_data_unwrapped[\"fPt\"].to_numpy(), save_dir=save_dir), save_path=save_dir, thumbnail_width=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
