{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel two sample test\n",
    "Kernel two sample test to determine if samples are drawn from the same distribution using Maximum Mean Discrepancy (mmd) statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the project directory\n",
    "%cd ../..\n",
    "# working directory should be ../pdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "module_path = os.path.abspath('src')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_period1 = pd.read_csv('csv_filepath_1', sep=\",\", index_col=0)\n",
    "data_period2 = pd.read_csv('csv_filepath_2', sep=\",\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_period1.shape)\n",
    "print(data_period2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def mmd(samples, gamma=None, alpha=0.05):\n",
    "    \"\"\"Compute unbiased MMD^2 estimate between two samples, with a hypothesis test threshold.\n",
    "    \n",
    "    Args:\n",
    "        X -- sample from distribution P\n",
    "        Y -- sample from distribution Q\n",
    "        gamma (float, optional) -- RBF kernel parameter (if None, use median heuristic)\n",
    "        alpha (float optional) -- significanece level for the hypothesis test (default: 0.05)\n",
    "    \n",
    "    Returns:\n",
    "        md_squared (float)\n",
    "        threshold (float) -- threshold for the null hypothesis at significance level `alpha`.\n",
    "        If mmd_squared > threshold, reject the null hypothesis (that samples are taken from the same distribution).\n",
    "    \"\"\"\n",
    "    X, Y = samples \n",
    "\n",
    "    if X is None or Y is None:\n",
    "        return None, None\n",
    "\n",
    "    m = len(X)\n",
    "    assert len(Y) == m, \"X and Y must have the same number of samples\"\n",
    "    \n",
    "    if gamma is None:\n",
    "        pairwise_dist = metrics.euclidean_distances(np.vstack([X, Y]), squared=True)\n",
    "        gamma = 1.0 / np.median(pairwise_dist[np.triu_indices_from(pairwise_dist, k=1)])\n",
    "    \n",
    "    Kxx = metrics.pairwise.rbf_kernel(X, gamma=gamma)\n",
    "    Kyy = metrics.pairwise.rbf_kernel(Y, gamma=gamma)\n",
    "    Kxy = metrics.pairwise.rbf_kernel(X, Y, gamma=gamma)\n",
    "\n",
    "    np.fill_diagonal(Kxx, 0)\n",
    "    np.fill_diagonal(Kyy, 0)\n",
    "\n",
    "    term1 = Kxx + Kyy - Kxy - Kxy.T\n",
    "    mmd_squared = np.sum(term1) / (m * (m - 1))\n",
    "    K = 1   # rbf kernel upper bound\n",
    "    threshold = (4 * K / np.sqrt(m)) * np.sqrt(np.log(1 / alpha))\n",
    "    \n",
    "    return mmd_squared, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of sets for different combinations of missing data\n",
    "def prepare_sets(data):\n",
    "\n",
    "    data_sets = {\n",
    "        \"no_TOF_TRD\": data[data[\"fTOFSignal\"].isna() & data[\"fTRDPattern\"].isna()].drop(columns=[\"fTOFSignal\", \"fBeta\", \"fTRDPattern\", \"fTRDSignal\"]),\n",
    "        \"no_TRD\": data[data[\"fTRDPattern\"].isna() & data[\"fTOFSignal\"].notna()].drop(columns=[\"fTRDPattern\", \"fTRDSignal\"]),\n",
    "        \"no_TOF\": data[data[\"fTOFSignal\"].isna() & data[\"fTRDPattern\"].notna()].drop(columns=[\"fTOFSignal\", \"fBeta\"]),\n",
    "        \"no_missing\": data[data[\"fTRDPattern\"].notna() & data[\"fTOFSignal\"].notna()]\n",
    "    } \n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                       message=\"X has feature names, but.*was fitted without feature names\")\n",
    "\n",
    "def get_samples(data_period1, data_period2, size=10000):\n",
    "    \"\"\" Get samples from two data taking periods\n",
    "    Args:\n",
    "        data_period1\n",
    "        data_period2,\n",
    "        size (float) -- desired sample size, default: 10000\n",
    "\n",
    "    Returns:\n",
    "        X_sample\n",
    "        Y_sample\n",
    "    \"\"\"\n",
    "    if len(data_period1) == 0 or len(data_period2) == 0:\n",
    "            return None, None\n",
    "\n",
    "    data_period1 = data_period1.dropna()\n",
    "    data_period2 = data_period2.dropna()\n",
    "\n",
    "    X = data_period1\n",
    "    Y = data_period2\n",
    "\n",
    "    scaler = StandardScaler().fit(np.vstack([X, Y]))\n",
    "    X_std, Y_std = scaler.transform(X), scaler.transform(Y)\n",
    "\n",
    "    indices = np.random.choice(X_std.shape[0], size=min(size, min(len(X_std), len(Y_std))), replace=False)\n",
    "    X_sample = X_std[indices]\n",
    "\n",
    "    indices = np.random.choice(Y_std.shape[0], size=min(size, min(len(X_std), len(Y_std))), replace=False)\n",
    "    Y_sample = Y_std[indices]\n",
    "\n",
    "    return X_sample, Y_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mmd(data_sets_1, data_sets_2, sample_size = 10000, alpha = 0.05, gamma=None):\n",
    "    results = {\n",
    "        \"no_TOF_TRD\": mmd(get_samples(data_sets_1[\"no_TOF_TRD\"], data_sets_2[\"no_TOF_TRD\"], sample_size), gamma, alpha),\n",
    "        \"no_TOF\": mmd(get_samples(data_sets_1[\"no_TOF\"], data_sets_2[\"no_TOF\"], sample_size), gamma, alpha),\n",
    "        \"no_TRD\": mmd(get_samples(data_sets_1[\"no_TRD\"], data_sets_2[\"no_TRD\"], sample_size), gamma, alpha),\n",
    "        \"no_missing\": mmd(get_samples(data_sets_1[\"no_missing\"], data_sets_2[\"no_missing\"], sample_size), gamma, alpha)\n",
    "    }\n",
    "\n",
    "    for name, result in results.items():\n",
    "        print(f\"MMD for missing {name}, null hypothesis {'REJECTED' if result[0] > result[1] else 'not rejected'} (MMD score: {result[0]:.2g}, threshold: {result[1]:.2g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns if comparing experimental vs simulated\n",
    "if data_period1.shape[1] == 21 and data_period2.shape[1] == 19:\n",
    "    data_period1 = data_period1.drop(columns = [\"fIsPhysicalPrimary\", \"fPdgCode\"])\n",
    "elif data_period1.shape[1] == 19 and data_period2.shape[1] == 21:\n",
    "    data_period2 = data_period2.drop(columns = [\"fIsPhysicalPrimary\", \"fPdgCode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets_1 = prepare_sets(data_period1)\n",
    "data_sets_2 = prepare_sets(data_period2)\n",
    "\n",
    "evaluate_mmd(data_sets_1, data_sets_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate tests for different particle types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run only if testing simulated vs simulated distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdi.data.constants import TARGET_COLUMN\n",
    "from pdi.constants import TARGET_CODES, PARTICLES_DICT\n",
    "\n",
    "features = data_period1.drop(columns=[TARGET_COLUMN]).columns\n",
    "sample_size = 10000\n",
    "\n",
    "for code in TARGET_CODES:\n",
    "    particle_data_1 = data_period1[data_period1[TARGET_COLUMN] == code]\n",
    "    particle_data_2 = data_period2[data_period2[TARGET_COLUMN] == code]\n",
    "\n",
    "    part_sets_1 = prepare_sets(particle_data_1)\n",
    "    part_sets_2 = prepare_sets(particle_data_2)\n",
    "\n",
    "    print(\"MMD for\", PARTICLES_DICT[code], \"distribution: \")\n",
    "    evaluate_mmd(part_sets_1, part_sets_2)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
